```{r}
library(reticulate)

conda_create("r-tensorflow", packages = "python=3.11")
use_condaenv("r-tensorflow", required = TRUE)

py_install(c("tensorflow-macos", "tensorflow-metal", "keras"), pip = TRUE)

library(tensorflow)
library(keras)
print(tf$config$list_physical_devices())

```

```{r}

library(tidyverse)
library(sf)
library(caret)
library(xgboost)
library(randomForest)
library(glmnet)
library(SuperLearner)
library(tidytext)
library(tm)
library(osmdata)
library(tmap)
library(text2vec)
library(irlba)
library(tensorflow)
library(keras)


clean_text <- function(text) {
  text %>%
    tolower() %>%
    str_replace_all("[^[:alnum:] ]", " ") %>%
    str_squish()
}

# -----------------------------
# 2. UNIFICAR Y PREPROCESAR
# -----------------------------
data <- bind_rows(
  train %>% mutate(dataset = "train"),
  test %>% mutate(dataset = "test")
)

stopifnot(all(c("median_m2", "tm_mts", "sitp_100m", "ciclorutas_mts", "parque",
                "avenidas", "invasiones_100m") %in% names(data)))

data <- data %>%
  mutate(
    description_clean = clean_text(description),
    title_clean = clean_text(title),
    has_balcon = as.numeric(str_detect(description_clean, "balc|terraza")),
    has_chimenea = as.numeric(str_detect(description_clean, "chimenea")),
    has_gimnasio = as.numeric(str_detect(description_clean, "gimnasio")),
    has_piscina = as.numeric(str_detect(description_clean, "piscina")),
    has_duplex = as.numeric(str_detect(description_clean, "duplex")),
    surface_total = ifelse(surface_total > 500, NA, surface_total),
    surface_total = ifelse(is.na(surface_total), median_m2, surface_total),
    rooms_density = rooms / (surface_total + 1),
    covered_ratio = surface_covered / (surface_total + 1),
    years_since_construction = 2023 - year,
    transport_score = sitp_100m + tm_mts,
    environment_score = ciclorutas_mts + avenidas + parque + invasiones_100m,
    localidad_fct = fct_lump_n(as.factor(localidad), n = 10),
    price_millions = price / 1e6
  )

```

```{r}
# -----------------------------
# 3. TF-IDF + PCA CON IRLBA
# -----------------------------
it <- itoken(data$description_clean, progressbar = FALSE)
vocab <- create_vocabulary(it, stopwords = stopwords::stopwords("es")) %>%
  prune_vocabulary(term_count_min = 5)
vectorizer <- vocab_vectorizer(vocab)
dtm <- create_dtm(it, vectorizer)

# PCA eficiente con 50 componentes
tfidf <- TfIdf$new()
dtm_tfidf <- tfidf$fit_transform(dtm)
tfidf_pca <- prcomp_irlba(dtm_tfidf, n = 50)
X_text_pca <- as.data.frame(tfidf_pca$x)
colnames(X_text_pca) <- paste0("text_pca_", 1:ncol(X_text_pca))

data <- bind_cols(data, X_text_pca)
```

```{r}
# -----------------------------
# 4. SEPARAR TRAIN Y TEST
# -----------------------------
features <- c("surface_total", "rooms", "bedrooms", "bathrooms", "parqueaderos",
              "has_balcon", "has_chimenea", "has_gimnasio", "has_piscina", "has_duplex",
              "years_since_construction", "rooms_density", "covered_ratio",
              "transport_score", "environment_score",
              paste0("text_pca_", 1:50))

train_clean <- data %>%
  filter(dataset == "train") %>%
  st_drop_geometry() %>%
  select(all_of(features), localidad_fct, price_millions) %>%
  drop_na(price_millions)

test_clean <- data %>%
  filter(dataset == "test") %>%
  st_drop_geometry() %>%
  select(all_of(features), localidad_fct, property_id)
```

```{r}
# -----------------------------
# 5. PREPROCESAMIENTO NUMÉRICO + OHE
# -----------------------------

# Asegurarse de que localidad_fct no tenga NA explícitos
train_clean$localidad_fct <- fct_explicit_na(train_clean$localidad_fct, na_level = "missing")
test_clean$localidad_fct <- fct_explicit_na(test_clean$localidad_fct, na_level = "missing")

# Imputación + normalización de variables numéricas
preproc <- preProcess(train_clean[, features], method = c("medianImpute", "center", "scale", "YeoJohnson"))

# Aplicar transformación a train/test
X_train <- predict(preproc, train_clean[, features])
X_test <- predict(preproc, test_clean[, features])

# Verificación de NA antes de OHE
cat("NA en X_train antes de dummies:", anyNA(X_train), "\n")

# One-hot encoding de localidad
dummies <- dummyVars(~ localidad_fct, data = train_clean)
X_train_dummies <- predict(dummies, train_clean)
X_test_dummies <- predict(dummies, test_clean)

# Unión final
X_train <- cbind(X_train, X_train_dummies)
X_test <- cbind(X_test, X_test_dummies)

# Verificación final
cat("NA en X_train final:", anyNA(X_train), "\n")
cat("NA en X_test final:", anyNA(X_test), "\n")

# -----------------------------
# 6. MATRICES PARA XGBOOST
# -----------------------------
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = log1p(train_clean$price_millions))
dtest <- xgb.DMatrix(data = as.matrix(X_test))

```

```{r}
library(keras)
library(tensorflow)

# Convertir los datos a matrices numéricas (en caso de que no lo estén)
X_train_mat <- as.matrix(X_train)
X_test_mat <- as.matrix(X_test)
y_train <- train_clean$price_millions

# Definir el modelo
model <- keras_model_sequential() %>%
  layer_dense(units = 128, activation = "relu", input_shape = c(ncol(X_train_mat))) %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1)

# Compilar el modelo: usar pérdida MSE y optimizador Adam
model %>% compile(
  loss = "mse",
  optimizer = optimizer_adam(),
  metrics = list("mean_absolute_error")
)

# Resumen
summary(model)

# Entrenamiento
history <- model %>% keras::fit(
  x = X_train_mat,
  y = y_train,
  epochs = 10,
  batch_size = 32,
  validation_split = 0.2,
  callbacks = list(callback_early_stopping(monitor = "val_loss", patience = 10, restore_best_weights = TRUE)),
  verbose = 2
)

# Predicciones para test (predicción directa)
predictions <- model %>% predict(X_test_mat)

# Puedes revisar las primeras predicciones
head(predictions)
```

```{r}
submission <- data.frame(
  property_id = test_clean$property_id,
  price = as.vector(predictions * 1e6) 
)

# Guardar el CSV
write.csv(submission, file = "submission_RN_10epoc.csv", row.names = TRUE)

# Predicciones sobre training
preds_train <- model %>% predict(X_test_mat) %>% as.vector()
# preds_train_exp <- expm1(preds_train)

# Error absoluto medio (en millones)
mae <- mean(abs(preds_train - train_clean$price_millions))
print(paste("MAE en entrenamiento (millones):", round(mae, 2)))

```
